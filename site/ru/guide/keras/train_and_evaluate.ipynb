{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_and_evaluate.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A8MVXQUFkX3n"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "IcfrhafzkZbH",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zOSK2IBG6-9Z"
      },
      "source": [
        "# Обучение и оценка с Keras\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7s-G0Fajsvjn"
      },
      "source": [
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />Смотрите на TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/ru/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Запустите в Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/ru/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />Изучайте код на GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/ru/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Скачайте ноутбук</a>\n",
        "  </td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj66ZXAzrJC2",
        "colab_type": "text"
      },
      "source": [
        "Note: Вся информация в этом разделе переведена с помощью русскоговорящего Tensorflow сообщества на общественных началах. Поскольку этот перевод не является официальным, мы не гарантируем что он на 100% аккуратен и соответствует [официальной документации на английском языке](https://www.tensorflow.org/?hl=en). Если у вас есть предложение как исправить этот перевод, мы будем очень рады увидеть pull request в [tensorflow/docs](https://github.com/tensorflow/docs) репозиторий GitHub. Если вы хотите помочь сделать документацию по Tensorflow лучше (сделать сам перевод или проверить перевод подготовленный кем-то другим), напишите нам на [docs-ru@tensorflow.org list](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-ru)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tc92ghg-sw2w"
      },
      "source": [
        "\n",
        "Это руководство охватывает обучение, оценку и прогнозирование (выводы) моделей в TensorFlow 2.0 в двух общих ситуациях:\n",
        "\n",
        "- При использовании встроенных API для обучения и валидации (таких как `model.fit()`, `model.evaluate()`, `model.predict()`). Этому посвящен раздел **\"Использование встроенных циклов обучения и оценки\"**.\n",
        "- При написании пользовательских циклов с нуля с использованием eager execution и объекта `GradientTape`. Эти вопросы рассматриваются в разделе **\"Написание собственных циклов обучения и оценки с нуля\"**.\n",
        "\n",
        "В целом, независимо от того, используете ли вы встроенные циклы или пишете свои собственные, обучение и оценка моделей работает строго одинаково для всех видов моделей Keras: Sequential моделей, созданных с помощью Functional API, и написанных с нуля с использованием субклассирования моделей.\n",
        "\n",
        "Это руководство не охватывает распределенное обучение."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RNG8OH3yuWrb"
      },
      "source": [
        "## Установка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uh_b0kqEuYah",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.backend.clear_session()  # Для простого сброса состояния ноутбука."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "052DbsQ175lP"
      },
      "source": [
        "## Часть I: Использование встроенных циклов обучения и оценки\n",
        "\n",
        "При передаче данных во встроенные обучающие циклы модели вы должны либо использовать **массивы Numpy** (если ваши данные малы и умещаются в памяти), либо объекты **tf.data Dataset**. В следующих нескольких параграфах мы будем использовать набор данных MNIST в качестве массива Numpy, чтобы показать, как использовать оптимизаторы, функции потерь и метрики."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rXDWPajQ8V8V"
      },
      "source": [
        "### Обзор API: первый полный пример\n",
        "\n",
        "Давайте рассмотрим следующую модель (здесь мы строим ее с помощью Functional API, но она может быть и Sequential или субклассированной моделью):\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_eJxUppsNjMj",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C02iiuYZNklq"
      },
      "source": [
        "Вот как выглядит типичный сквозной процесс работы, состоящий из обучения, проверки на отложенных данных, сгенерированных из исходных данных обучения, и, наконец, оценки на тестовых данных:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTzpaClpNvyJ",
        "colab": {}
      },
      "source": [
        "# Загрузим учебный датасет для этого примера\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "# Предобработаем данные (это массивы Numpy)\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
        "\n",
        "y_train = y_train.astype('float32')\n",
        "y_test = y_test.astype('float32')\n",
        "\n",
        "# Зарезервируем 10,000 примеров для валидации\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "# Укажем конфигурацию обучения (оптимизатор, функция потерь, метрики)\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(),  # Оптимизатор\n",
        "              # Минимизируемая функция потерь\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              # Список метрик для мониторинга\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "# Обучим модель разбив данные на \"пакеты\"\n",
        "# размером \"batch_size\", и последовательно итерируя\n",
        "# весь датасет заданное количество \"эпох\"\n",
        "print('# Обучаем модель на тестовых данных')\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=3,\n",
        "                    # Мы передаем валидационные данные для\n",
        "                    # мониторинга потерь и метрик на этих данных\n",
        "                    # в конце каждой эпохи\n",
        "                    validation_data=(x_val, y_val))\n",
        "\n",
        "# Возвращаемый объект \"history\" содержит записи\n",
        "# значений потерь и метрик во время обучения\n",
        "print('\\nhistory dict:', history.history)\n",
        "\n",
        "# Оценим модель на тестовых данных, используя `evaluate`\n",
        "print('\\n# Оцениваем на тестовых данных')\n",
        "results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "print('test loss, test acc:', results)\n",
        "\n",
        "# Сгенерируем прогнозы (вероятности -- выходные данные последнего слоя)\n",
        "# на новых данных с помощью `predict`\n",
        "print('\\n# Генерируем прогнозы для 3 образцов')\n",
        "predictions = model.predict(x_test[:3])\n",
        "print('размерность прогнозов:', predictions.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R43f-GWfDkQ5"
      },
      "source": [
        "### Определение потерь, метрик и оптимизатора\n",
        "\n",
        "Для обучения модели с помощью `fit`, вам нужно задать функцию потерь, оптимизатор, и опционально некоторые метрики для мониторинга.\n",
        "\n",
        "Вам нужно передать их модели в качестве аргументов метода `compile()`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CiB85T_hM_vQ",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lq3wsghNM_7C"
      },
      "source": [
        "Аргумент `metrics` должен быть списком -- ваша модель может иметь любое количество метрик.\n",
        "\n",
        "Если у вашей модели несколько выходов, вы можете задать различные функции потерь и метрики для каждого выхода,\n",
        "и вы можете регулировать вклад каждого выхода в общее значение потерь модели. Вы найдете больше деталей об этом в разделе \"**Передача данных в модели с несколькими входами и выходами**\".\n",
        "\n",
        "Обратите внимание, что во многих случаях потери и метрики задаются с помощью строковых идентификаторов:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pOQOcScsNiT3",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IVHOVTqFknZP"
      },
      "source": [
        "Для последующего переиспользования поместим определение нашей модели и шаг компиляции в функции; мы будем вызывать их несколько раз в разных примерах этого руководства."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G3Zrjsf_kt8k",
        "colab": {}
      },
      "source": [
        "def get_uncompiled_model():\n",
        "  inputs = keras.Input(shape=(784,), name='digits')\n",
        "  x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "  x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "  outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "def get_compiled_model():\n",
        "  model = get_uncompiled_model()\n",
        "  model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1rDyyQEtC8E"
      },
      "source": [
        "#### Вам доступно множество встроенных оптимизаторов, функций потерь и метрик\n",
        "\n",
        "Как правило, вам не нужно создавать с нуля собственные функции потерь, метрики, или оптимизаторы, поскольку то, что вам нужно, скорее всего, уже является частью Keras API:\n",
        "\n",
        "Оптимизаторы:\n",
        "- `SGD()` (с или без momentum)\n",
        "- `RMSprop()`\n",
        "- `Adam()`\n",
        "- и т.д.\n",
        "\n",
        "Потери:\n",
        "- `MeanSquaredError()`\n",
        "- `KLDivergence()`\n",
        "- `CosineSimilarity()`\n",
        "- и т.д.\n",
        "\n",
        "Метрики:\n",
        "- `AUC()`\n",
        "- `Precision()`\n",
        "- `Recall()`\n",
        "- и т.д."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LH1QjaltNm2U"
      },
      "source": [
        "#### Кастомные функции потерь\n",
        "\n",
        "Есть два способа обеспечить кастомные функции потерь с Keras. В примере создается функция принимающая на вход `y_true` и `y_pred`. Следующий пример показывает функцию потерь вычисляющую среднее расстояние между реальными данными и прогнозами:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IIn3_iD0Nyll",
        "colab": {}
      },
      "source": [
        "def basic_loss_function(y_true, y_pred):\n",
        "    return tf.math.reduce_mean(y_true - y_pred)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=basic_loss_function)\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZqpsybNN_Wh"
      },
      "source": [
        "Если вам нужна функция потерь у которой есть иные параметры кроме `y_true` и `y_pred`, вы можете субклассировать класс `tf.keras.losses.Loss` и реализовать следующие два метода:\n",
        "\n",
        "* `__init__(self)` —Принять параметры, передаваемые при вызове вашей функции потерь \n",
        "* `call(self, y_true, y_pred)` —Использовать цели (`y_true`) и предсказания модели (`y_pred`) для вычисления потерь модели\n",
        "\n",
        "Параметры передаваемые в `__init__()` могут быть использованы во время `call()` при вычислении потерь.\n",
        "\n",
        "Следующий пример показывает как реализовать функцию потерь `WeightedCrossEntropy` которая вычисляет `BinaryCrossEntropy`, где потери конкретного класса или всей функции могут быть модифицированы при помощи скаляра."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gr996axuOCfX",
        "colab": {}
      },
      "source": [
        "class WeightedBinaryCrossEntropy(keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      pos_weight: Скалярный вес для положительных меток функции потерь.\n",
        "      weight: Скалярный вес для всей функции потерь.\n",
        "      from_logits: Вычислять ли потери от логитов или вероятностей.\n",
        "      reduction: Тип tf.keras.losses.Reduction для применения к функции потерь.\n",
        "      name: Имя функции потерь.\n",
        "    \"\"\"\n",
        "    def __init__(self, pos_weight, weight, from_logits=False,\n",
        "                 reduction=keras.losses.Reduction.AUTO,\n",
        "                 name='weighted_binary_crossentropy'):\n",
        "        super(WeightedBinaryCrossEntropy, self).__init__(reduction=reduction,\n",
        "                                                         name=name)\n",
        "        self.pos_weight = pos_weight\n",
        "        self.weight = weight\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        if not self.from_logits:\n",
        "            # Вручную посчитаем взвешенную кросс-энтропию.\n",
        "            # Формула следующая qz * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n",
        "            # где z - метки, x - логиты, а q - веса.\n",
        "            # Поскольку переданные значения от сигмоиды (предположим в этом случае)\n",
        "            # sigmoid(x) будет заменено y_pred\n",
        "\n",
        "            # qz * -log(sigmoid(x)) 1e-6 добавляется как эпсилон, чтобы не передать нуль в логарифм\n",
        "            x_1 = y_true * self.pos_weight * -tf.math.log(y_pred + 1e-6)\n",
        "\n",
        "            # (1 - z) * -log(1 - sigmoid(x)). Добавляем эпсилон, чтобы не пропустить нуль в логарифм\n",
        "            x_2 = (1 - y_true) * -tf.math.log(1 - y_pred + 1e-6)\n",
        "\n",
        "            return tf.add(x_1, x_2) * self.weight \n",
        "\n",
        "        # Use built in function\n",
        "        return tf.nn.weighted_cross_entropy_with_logits(y_true, y_pred, self.pos_weight) * self.weight\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=WeightedBinaryCrossEntropy(0.5, 2))\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YUkJi7ENDnCg"
      },
      "source": [
        "#### Кастомные метрики\n",
        "\n",
        "Если вам нужны метрики не являющиеся частью API, вы можете легко создать кастомные метрики субклассировав класс `Metric`. Вам нужно реализовать 4 метода:\n",
        "\n",
        "- `__init__(self)`,  в котором вы создадите переменные состояния для своей метрики.\n",
        "- `update_state(self, y_true, y_pred, sample_weight=None)`, который использует ответы `y_true` и предсказания модели `y_pred` для обновления переменных состояния.\n",
        "- `result(self)`, использующий переменные состояния для вычисления конечнного результата.\n",
        "- `reset_states(self)`, который переинициализирует состояние метрики.\n",
        "\n",
        "Обновление состояния и вычисление результатов хранятся отдельно (в `update_state()` и `result()` соответственно), потому что в некоторых случаях вычисление результатов может быть очень дорогим и будет выполняться только периодически.\n",
        "\n",
        "Вот простой пример, показывающий, как реализовать метрику `CategoricalTruePositives`, которая считает сколько элементов были правильно классифицированы, как принадлежащие к данному классу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MbSNO6uePVsF",
        "colab": {}
      },
      "source": [
        "class CategoricalTruePositives(keras.metrics.Metric):\n",
        "\n",
        "    def __init__(self, name='categorical_true_positives', **kwargs):\n",
        "      super(CategoricalTruePositives, self).__init__(name=name, **kwargs)\n",
        "      self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "      y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n",
        "      values = tf.cast(y_true, 'int32') == tf.cast(y_pred, 'int32')\n",
        "      values = tf.cast(values, 'float32')\n",
        "      if sample_weight is not None:\n",
        "        sample_weight = tf.cast(sample_weight, 'float32')\n",
        "        values = tf.multiply(values, sample_weight)\n",
        "      self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "    def result(self):\n",
        "      return self.true_positives\n",
        "\n",
        "    def reset_states(self):\n",
        "      # Состояние метрики будет сброшено в начале каждой эпохи.\n",
        "      self.true_positives.assign(0.)\n",
        "\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[CategoricalTruePositives()])\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HipGsW7qDo0Q"
      },
      "source": [
        "#### Обработка функций потерь и метрик, не соответствующих стандартной сигнатуре\n",
        "\n",
        "Подавляющее большинство потерь и метрик может быть вычислено из` y_true` и `y_pred`, где` y_pred` - вывод вашей модели. Но не все. Например, потери регуляризации могут требовать только активацию слоя (в этом случае нет целевых значений), и эта активация может не являться выходом модели.\n",
        "\n",
        "В таких случаях вы можете вызвать `self.add_loss(loss_value)` из метода `call` кастомного слоя. Вот простой пример, который добавляет регуляризацию активности (отметим что регуляризация активности встроена во все слои Keras  -- этот слой используется только для приведения конкретного примера):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CrYwrDR0PWab",
        "colab": {}
      },
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "\n",
        "  def call(self, inputs):\n",
        "    self.add_loss(tf.reduce_sum(inputs) * 0.1)\n",
        "    return inputs  # Проходной слой.\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "\n",
        "# Вставим регуляризацию активности в качестве слоя\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Полученные потери будут намного больше чем раньше\n",
        "# из-за компонента регуляризации.\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fWnqsdwt06us"
      },
      "source": [
        "Вы можете сделать то же самое для логирования значений метрик:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBYYhVgJ0973",
        "colab": {}
      },
      "source": [
        "class MetricLoggingLayer(layers.Layer):\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Аргумент `aggregation` определяет\n",
        "    # как аггрегировать попакетные значения\n",
        "    # в каждой эпохе:\n",
        "    # в этом случае мы просто усредняем их.\n",
        "    self.add_metric(keras.backend.std(inputs),\n",
        "                    name='std_of_activation',\n",
        "                    aggregation='mean')\n",
        "    return inputs  # Проходной слой.\n",
        "\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "\n",
        "# Вставка логирования std в качестве слоя.\n",
        "x = MetricLoggingLayer()(x)\n",
        "\n",
        "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
        "              loss='sparse_categorical_crossentropy')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f9_9XQvA0Jap"
      },
      "source": [
        "В [Functional API](functional.ipynb), вы можете также вызвать `model.add_loss(loss_tensor)`, или `model.add_metric(metric_tensor, name, aggregation)`.\n",
        "\n",
        "Вот простой пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrA1yFql0gXg",
        "colab": {}
      },
      "source": [
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x1 = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "x2 = layers.Dense(64, activation='relu', name='dense_2')(x1)\n",
        "outputs = layers.Dense(10, activation='softmax', name='predictions')(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.add_loss(tf.reduce_sum(x1) * 0.1)\n",
        "\n",
        "model.add_metric(keras.backend.std(x1),\n",
        "                 name='std_of_activation',\n",
        "                 aggregation='mean')\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "              loss='sparse_categorical_crossentropy')\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=64,\n",
        "          epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJqpjGyMAa82"
      },
      "source": [
        "\n",
        "#### Автоматическое выделение валидационного отложенного множества\n",
        "\n",
        "В первом полном примере, как вы видели, мы использовали аргумент `validation_data` для передачи кортежа\n",
        "массивов Numpy `(x_val, y_val)` модели для оценки валидационных потерь и метрик в конце каждой эпохи.\n",
        "\n",
        "Вот другая опция: аргумент `validation_split` позволяет вам автоматически зарезервировать часть ваших тренировочных данных для валидации. Значением аргумента является доля данных, которые должны быть зарезервированы для валидации, поэтому значение должно быть больше 0 и меньше 1. Например, `validation_split=0.2` значит \"используйте 20% данных для валидации\", а `validation_split=0.6` значит \"используйте 60% данных для валидации\".\n",
        "\n",
        "Валидация вычисляется *взятием последних x% записей массивов полученных вызовом `fit`, перед любым перемешиванием*.\n",
        "\n",
        "Вы можете использовать `validation_split` только когда обучаете данными Numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z-DQyeRePYS-",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train, batch_size=64, validation_split=0.2, epochs=1, steps_per_epoch=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uq0TDb15DBbc"
      },
      "source": [
        "### Обучение и оценка с tf.data Dataset\n",
        "\n",
        "В последних нескольких параграфах вы видели, как обрабатывать потери, метрики и оптимизаторы, и посмотрели, как использовать аргументы `validation_data` и `validation_split` в `fit`, когда ваши данные передаются в виде массивов Numpy.\n",
        "\n",
        "Давайте теперь рассмотрим случай, когда ваши даннные поступают в форме tf.data Dataset.\n",
        "\n",
        "tf.data API это набор утилит в TensorFlow 2.0 для загрузки и предобработки данных быстрым и масштабируемым способом.\n",
        "\n",
        "Для полного руководства по созданию Dataset-ов, см. [документацию tf.data](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "Вы можете передать экземпляр Dataset напрямую в методы `fit()`, `evaluate()` и `predict()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iY27EJsmENBj",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Сперва давайте создадим экземпляр тренировочного Dataset.\n",
        "# Для нашего примера мы будем использовать те же данные MNIST что и ранее.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# Перемешаем и нарежем набор данных.\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Сейчас получим тестовый датасет.\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(64)\n",
        "\n",
        "# Поскольку датасет уже позаботился о разбивке на пакеты,\n",
        "# мы не передаем аргумент `batch_size`.\n",
        "model.fit(train_dataset, epochs=3)\n",
        "\n",
        "# Вы можете также оценить модель или сделать прогнозы на датасете.\n",
        "print('\\n# Оценка')\n",
        "model.evaluate(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r2yh2h79ENUy"
      },
      "source": [
        "Заметьте, что Dataset сбрасывается в конце каждой эпохи, поэтому он может быть переиспользован в следующей эпохе.\n",
        "\n",
        "Если вы хотите учиться только на определенном количестве пакетов из этого Dataset, вы можете передать аргумент `steps_per_epoch`, который указывает, сколько шагов обучения должна выполнить модель, используя этот Dataset, прежде чем перейти к следующей эпохе.\n",
        "\n",
        "В этом случае датасет не будет сброшен в конце каждой эпохи, вместо этого мы просто продолжим обрабаатывать следующие пакеты. В датасете в конечном счете закончатся данные (если только это не зацикленный бесконечно датасет)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7t0V6K_EEdB7",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Подготовка учебного датасета\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Использовать только 100 пакетов за эпоху (это 64 * 100 примеров)\n",
        "model.fit(train_dataset.take(100), epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_e3b3rSmC-fL"
      },
      "source": [
        "#### Использование валидационного датасета\n",
        "\n",
        "Вы можете передать экземпляр Dataset как аргумент `validation_data` в `fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pv_53VE2C-11",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=3, validation_data=val_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J2Z7pgmSC7Zk"
      },
      "source": [
        "At the end of each epoch, the model will iterate over the validation Dataset and compute the validation loss and validation metrics.\n",
        "\n",
        "If you want to run validation only on a specific number of batches from this Dataset, you can pass the `validation_steps` argument, which specifies how many validation steps the model should run with the validation Dataset before interrupting validation and moving on to the next epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eGa4DQA0C8Mu",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "# Prepare the training dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# Prepare the validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=3,\n",
        "          # Only run validation using the first 10 batches of the dataset\n",
        "          # using the `validation_steps` argument\n",
        "          validation_data=val_dataset, validation_steps=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6qo-fWETC4sD"
      },
      "source": [
        "Note that the validation Dataset will be reset after each use (so that you will always be evaluating on the same samples from epoch to epoch).\n",
        "\n",
        "The argument `validation_split` (generating a holdout set from the training data) is not supported when training from Dataset objects, since this features requires the ability to index the samples of the datasets, which is not possible in general with the Dataset API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIGt6NNqC2ZR"
      },
      "source": [
        "\n",
        "### Other input formats supported\n",
        "\n",
        "Besides Numpy arrays and TensorFlow Datasets, it's possible to train a Keras model using Pandas dataframes, or from Python generators that yield batches.\n",
        "\n",
        "In general, we recommend that you use Numpy input data if your data is small and fits in memory, and Datasets otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bid1XnTl_zNT"
      },
      "source": [
        "### Using sample weighting and class weighting\n",
        "\n",
        "Besides input data and target data, it is possible to pass sample weights or class weights to a model when using `fit`:\n",
        "\n",
        "- When training from Numpy data: via the `sample_weight` and `class_weight` arguments.\n",
        "- When training from Datasets: by having the Dataset return a tuple `(input_batch, target_batch, sample_weight_batch)` .\n",
        "\n",
        "A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes). When the weights used are ones and zeros, the array can be used as a *mask* for the loss function (entirely discarding the contribution of certain samples to the total loss).\n",
        "\n",
        "A \"class weights\" dict is a more specific instance of the same concept: it maps class indices to the sample weight that should be used for samples belonging to this class. For instance, if class \"0\" is twice less represented than class \"1\" in your data, you could use `class_weight={0: 1., 1: 0.5}`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rbH4jT5Hp_gg"
      },
      "source": [
        "Here's a Numpy example where we use class weights or sample weights to give more importance to the correct classification of class #5 (which is the digit \"5\" in the MNIST dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Y7QBNUXWTva",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class_weight = {0: 1., 1: 1., 2: 1., 3: 1., 4: 1.,\n",
        "                # Set weight \"2\" for class \"5\",\n",
        "                # making this class 2x more important\n",
        "                5: 2.,\n",
        "                6: 1., 7: 1., 8: 1., 9: 1.}\n",
        "print('Fit with class weight')\n",
        "model.fit(x_train, y_train,\n",
        "          class_weight=class_weight,\n",
        "          batch_size=64,\n",
        "          epochs=4)\n",
        "\n",
        "# Here's the same example using `sample_weight` instead:\n",
        "sample_weight = np.ones(shape=(len(y_train),))\n",
        "sample_weight[y_train == 5] = 2.\n",
        "print('\\nFit with sample weight')\n",
        "\n",
        "model = get_compiled_model()\n",
        "model.fit(x_train, y_train,\n",
        "          sample_weight=sample_weight,\n",
        "          batch_size=64,\n",
        "          epochs=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qf1KvM0pqBaV"
      },
      "source": [
        "Here's a matching Dataset example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LHD60GshxSpf",
        "colab": {}
      },
      "source": [
        "sample_weight = np.ones(shape=(len(y_train),))\n",
        "sample_weight[y_train == 5] = 2.\n",
        "\n",
        "# Create a Dataset that includes sample weights\n",
        "# (3rd element in the return tuple).\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train, y_train, sample_weight))\n",
        "\n",
        "# Shuffle and slice the dataset.\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "model = get_compiled_model()\n",
        "model.fit(train_dataset, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jXt0xQvDxMeq"
      },
      "source": [
        "### Passing data to multi-input, multi-output models\n",
        "\n",
        "In the previous examples, we were considering a model with a single input (a tensor of shape `(764,)`) and a single output (a prediction tensor of shape `(10,)`). But what about models that have multiple inputs or outputs?\n",
        "\n",
        "Consider the following model, which has an image input of shape `(32, 32, 3)` (that's `(height, width, channels)`) and a timeseries input of shape `(None, 10)` (that's `(timesteps, features)`). Our model will have two outputs computed from the combination of these inputs: a \"score\" (of shape `(1,)`) and a probability distribution over five classes (of shape `(5,)`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QNUSGfKq1cZ-",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "image_input = keras.Input(shape=(32, 32, 3), name='img_input')\n",
        "timeseries_input = keras.Input(shape=(None, 10), name='ts_input')\n",
        "\n",
        "x1 = layers.Conv2D(3, 3)(image_input)\n",
        "x1 = layers.GlobalMaxPooling2D()(x1)\n",
        "\n",
        "x2 = layers.Conv1D(3, 3)(timeseries_input)\n",
        "x2 = layers.GlobalMaxPooling1D()(x2)\n",
        "\n",
        "x = layers.concatenate([x1, x2])\n",
        "\n",
        "score_output = layers.Dense(1, name='score_output')(x)\n",
        "class_output = layers.Dense(5, activation='softmax', name='class_output')(x)\n",
        "\n",
        "model = keras.Model(inputs=[image_input, timeseries_input],\n",
        "                    outputs=[score_output, class_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cIWhMd7p3NGp"
      },
      "source": [
        "Let's plot this model, so you can clearly see what we're doing here (note that the shapes shown in the plot are batch shapes, rather than per-sample shapes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gKVQ4Y573Q_c",
        "colab": {}
      },
      "source": [
        "keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vAauLxiT278G"
      },
      "source": [
        "At compilation time, we can specify different losses to different outputs, by passing the loss functions as a list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vDa7JSz93phE",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(),\n",
        "          keras.losses.CategoricalCrossentropy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bQip1Fex4-D5"
      },
      "source": [
        "If we only passed a single loss function to the model, the same loss function would be applied to every output, which is not appropriate here.\n",
        "\n",
        "Likewise for metrics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tkl5LMGi4_gK",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(),\n",
        "          keras.losses.CategoricalCrossentropy()],\n",
        "    metrics=[[keras.metrics.MeanAbsolutePercentageError(),\n",
        "              keras.metrics.MeanAbsoluteError()],\n",
        "             [keras.metrics.CategoricalAccuracy()]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w_WWk9b1374P"
      },
      "source": [
        "Since we gave names to our output layers, we could also specify per-output losses and metrics via a dict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1MEctLBD4APc",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
        "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
        "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
        "                              keras.metrics.MeanAbsoluteError()],\n",
        "             'class_output': [keras.metrics.CategoricalAccuracy()]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NVXRX0PX6lIn"
      },
      "source": [
        "We recommend the use of explicit names and dicts if you have more than 2 outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RX3FTDWw4BOA"
      },
      "source": [
        "It's possible to give different weights to different output-specific losses (for instance, one might wish to privilege the \"score\" loss in our example, by giving to 2x the importance of the class loss), using the `loss_weights` argument:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgVK3xAv4JNv",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={'score_output': keras.losses.MeanSquaredError(),\n",
        "          'class_output': keras.losses.CategoricalCrossentropy()},\n",
        "    metrics={'score_output': [keras.metrics.MeanAbsolutePercentageError(),\n",
        "                              keras.metrics.MeanAbsoluteError()],\n",
        "             'class_output': [keras.metrics.CategoricalAccuracy()]},\n",
        "    loss_weights={'score_output': 2., 'class_output': 1.})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vd8VzIu-3p0n"
      },
      "source": [
        "You could also chose not to compute a loss for certain outputs, if these outputs meant for prediction but not for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aUyohARI4Kqn",
        "colab": {}
      },
      "source": [
        "# List loss version\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[None, keras.losses.CategoricalCrossentropy()])\n",
        "\n",
        "# Or dict loss version\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss={'class_output': keras.losses.CategoricalCrossentropy()})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n0MwZc184MG_"
      },
      "source": [
        "Passing data to a multi-input or multi-output model in `fit` works in a similar way as specifying a loss function in `compile`:\n",
        "you can pass *lists of Numpy arrays (with 1:1 mapping to the outputs that received a loss function)* or *dicts mapping output names to Numpy arrays of training data*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "So4cYwSW4la8",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
        "    loss=[keras.losses.MeanSquaredError(),\n",
        "          keras.losses.CategoricalCrossentropy()])\n",
        "\n",
        "# Generate dummy Numpy data\n",
        "img_data = np.random.random_sample(size=(100, 32, 32, 3))\n",
        "ts_data = np.random.random_sample(size=(100, 20, 10))\n",
        "score_targets = np.random.random_sample(size=(100, 1))\n",
        "class_targets = np.random.random_sample(size=(100, 5))\n",
        "\n",
        "# Fit on lists\n",
        "model.fit([img_data, ts_data], [score_targets, class_targets],\n",
        "          batch_size=32,\n",
        "          epochs=3)\n",
        "\n",
        "# Alternatively, fit on dicts\n",
        "model.fit({'img_input': img_data, 'ts_input': ts_data},\n",
        "          {'score_output': score_targets, 'class_output': class_targets},\n",
        "          batch_size=32,\n",
        "          epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m7Ah1QUf4ll6"
      },
      "source": [
        "Here's the Dataset use case: similarly as what we did for Numpy arrays, the Dataset should return\n",
        "a tuple of dicts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vx34-FDZ4tdJ",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    ({'img_input': img_data, 'ts_input': ts_data},\n",
        "     {'score_output': score_targets, 'class_output': class_targets}))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "model.fit(train_dataset, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y_gqVFcHAAga"
      },
      "source": [
        "### Using callbacks\n",
        "\n",
        "Callbacks in Keras are objects that are called at different point during training (at the start of an epoch, at the end of a batch, at the end of an epoch, etc.) and which can be used to implement behaviors such as:\n",
        "\n",
        "- Doing validation at different points during training (beyond the built-in per-epoch validation)\n",
        "- Checkpointing the model at regular intervals or when it exceeds a certain accuracy threshold\n",
        "- Changing the learning rate of the model when training seems to be plateauing\n",
        "- Doing fine-tuning of the top layers when training seems to be plateauing\n",
        "- Sending email or instant message notifications when training ends or where a certain performance threshold is exceeded\n",
        "- Etc.\n",
        "\n",
        "Callbacks can be passed as a list to your call to `fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pNEhXWcnSG9B",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor='val_loss',\n",
        "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "        min_delta=1e-2,\n",
        "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        patience=2,\n",
        "        verbose=1)\n",
        "]\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=64,\n",
        "          callbacks=callbacks,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PE3VK3WpSHL_"
      },
      "source": [
        "#### Many built-in callbacks are available\n",
        "\n",
        "- `ModelCheckpoint`: Periodically save the model.\n",
        "- `EarlyStopping`: Stop training when training is no longer improving the validation metrics.\n",
        "- `TensorBoard`: periodically write model logs that can be visualized in TensorBoard (more details in the section \"Visualization\").\n",
        "- `CSVLogger`: streams loss and metrics data to a CSV file.\n",
        "- etc.\n",
        "\n",
        "\n",
        "\n",
        "#### Writing your own callback\n",
        "\n",
        "You can create a custom callback by extending the base class keras.callbacks.Callback. A callback has access to its associated model through the class property `self.model`.\n",
        "\n",
        "Here's a simple example saving a list of per-batch loss values during training:\n",
        "\n",
        "```python\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "\n",
        "    def on_train_begin(self, logs):\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.losses.append(logs.get('loss'))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mC08S-e-yOd7"
      },
      "source": [
        "### Checkpointing models\n",
        "\n",
        "When you're training model on relatively large datasets, it's crucial to save checkpoints of your model at frequent intervals.\n",
        "\n",
        "The easiest way to achieve this is with the `ModelCheckpoint` callback:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yEER-qc-xXNC",
        "colab": {}
      },
      "source": [
        "model = get_compiled_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath='mymodel_{epoch}.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_loss` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        verbose=1)\n",
        "]\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=3,\n",
        "          batch_size=64,\n",
        "          callbacks=callbacks,\n",
        "          validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E1AZEf3ixXm8"
      },
      "source": [
        "You call also write your own callback for saving and restoring models.\n",
        "\n",
        "For a complete guide on serialization and saving, see [Guide to Saving and Serializing Models](./save_and_serialize.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j22FtN-z_-Wj"
      },
      "source": [
        "### Using learning rate schedules\n",
        "\n",
        "A common pattern when training deep learning models is to gradually reduce the learning as training progresses. This is generally known as \"learning rate decay\".\n",
        "\n",
        "The learning decay schedule could be static (fixed in advance, as a function of the current epoch or the current batch index), or dynamic (responding to the current behavior of the model, in particular the validation loss).\n",
        "\n",
        "#### Passing a schedule to an optimizer\n",
        "\n",
        "You can easily use a static learning rate decay schedule by passing a schedule object as the `learning_rate` argument in your optimizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cu-pnB_ctEav",
        "colab": {}
      },
      "source": [
        "initial_learning_rate = 0.1\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=100000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=True)\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Ltf0Xtrtw-2"
      },
      "source": [
        "Several built-in schedules are available: `ExponentialDecay`, `PiecewiseConstantDecay`, `PolynomialDecay`, and `InverseTimeDecay`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "13CvYhBptEjh"
      },
      "source": [
        "#### Using callbacks to implement a dynamic learning rate schedule\n",
        "\n",
        "A dynamic learning rate schedule (for instance, decreasing the learning rate when the validation loss is no longer improving) cannot be achieved with these schedule objects since the optimizer does not have access to validation metrics.\n",
        "\n",
        "However, callbacks do have access to all metrics, including validation metrics! You can thus achieve this pattern by using a callback that modifies the current learning rate on the optimizer. In fact, this is even built-in as the `ReduceLROnPlateau` callback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tnUhGslABdvx"
      },
      "source": [
        "### Visualizing loss and metrics during training\n",
        "\n",
        "The best way to keep an eye on your model during training is to use [TensorBoard](https://www.tensorflow.org/tensorboard), a browser-based application that you can run locally that provides you with:\n",
        "\n",
        "- Live plots of the loss and metrics for training and evaluation\n",
        "- (optionally) Visualizations of the histograms of your layer activations\n",
        "- (optionally) 3D visualizations of the embedding spaces learned by your `Embedding` layers\n",
        "\n",
        "If you have installed TensorFlow with pip, you should be able to launch TensorBoard from the command line:\n",
        "\n",
        "```\n",
        "tensorboard --logdir=/full_path_to_your_logs\n",
        "```\n",
        "\n",
        "#### Using the TensorBoard callback\n",
        "\n",
        "The easiest way to use TensorBoard with a Keras model and the `fit` method is the `TensorBoard` callback.\n",
        "\n",
        "In the simplest case, just specify where you want the callback to write logs, and you're good to go:\n",
        "\n",
        "```python\n",
        "tensorboard_cbk = keras.callbacks.TensorBoard(log_dir='/full_path_to_your_logs')\n",
        "model.fit(dataset, epochs=10, callbacks=[tensorboard_cbk])\n",
        "```\n",
        "\n",
        "The `TensorBoard` callback has many useful options, including whether to log embeddings, histograms, and how often to write logs:\n",
        "\n",
        "```python\n",
        "keras.callbacks.TensorBoard(\n",
        "  log_dir='/full_path_to_your_logs',\n",
        "  histogram_freq=0,  # How often to log histogram visualizations\n",
        "  embeddings_freq=0,  # How often to log embedding visualizations\n",
        "  update_freq='epoch')  # How often to write logs (default: once per epoch)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r5ZnFry7-B7"
      },
      "source": [
        "## Part II: Writing your own training & evaluation loops from scratch\n",
        "\n",
        "If you want lower-level over your training & evaluation loops than what `fit()` and `evaluate()` provide, you should write your own. It's actually pretty simple! But you should be ready to have a lot more debugging to do on your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HaRvt057_37D"
      },
      "source": [
        "### Using the GradientTape: a first end-to-end example\n",
        "\n",
        "Calling a model inside a `GradientTape` scope enables you to retrieve the gradients of the trainable weights of the layer with respect to a loss value. Using an optimizer instance, you can use these gradients to update these variables (which you can retrieve using `model.trainable_weights`).\n",
        "\n",
        "Let's reuse our initial MNIST model from Part I, and let's train it using mini-batch gradient with a custom training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XQqxREEL66Kg",
        "colab": {}
      },
      "source": [
        "# Get the model.\n",
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "outputs = layers.Dense(10, name='predictions')(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Iterate over epochs.\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "    # Open a GradientTape to record the operations run\n",
        "    # during the forward pass, which enables autodifferentiation.\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # Run the forward pass of the layer.\n",
        "      # The operations that the layer applies\n",
        "      # to its inputs are going to be recorded\n",
        "      # on the GradientTape.\n",
        "      logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "      # Compute the loss value for this minibatch.\n",
        "      loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "    # Use the gradient tape to automatically retrieve\n",
        "    # the gradients of the trainable variables with respect to the loss.\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "    # Run one step of gradient descent by updating\n",
        "    # the value of the variables to minimize the loss.\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    # Log every 200 batches.\n",
        "    if step % 200 == 0:\n",
        "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
        "        print('Seen so far: %s samples' % ((step + 1) * 64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1lt9Yva8xgCs"
      },
      "source": [
        "### Low-level handling of metrics\n",
        "\n",
        "Let's add metrics to the mix. You can readily reuse the built-in metrics (or custom ones you wrote) in such training loops written from scratch. Here's the flow:\n",
        "\n",
        "- Instantiate the metric at the start of the loop\n",
        "- Call `metric.update_state()` after each batch\n",
        "- Call `metric.result()` when you need to display the current value of the metric\n",
        "- Call `metric.reset_states()` when you need to clear the state of the metric (typically at the end of an epoch)\n",
        "\n",
        "Let's use this knowledge to compute `SparseCategoricalAccuracy` on validation data at the end of each epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JDKgp70UxwdR",
        "colab": {}
      },
      "source": [
        "# Get model\n",
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "\n",
        "# Iterate over epochs.\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(x_batch_train)\n",
        "      loss_value = loss_fn(y_batch_train, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    # Update training metric.\n",
        "    train_acc_metric(y_batch_train, logits)\n",
        "\n",
        "    # Log every 200 batches.\n",
        "    if step % 200 == 0:\n",
        "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
        "        print('Seen so far: %s samples' % ((step + 1) * 64))\n",
        "\n",
        "  # Display metrics at the end of each epoch.\n",
        "  train_acc = train_acc_metric.result()\n",
        "  print('Training acc over epoch: %s' % (float(train_acc),))\n",
        "  # Reset training metrics at the end of each epoch\n",
        "  train_acc_metric.reset_states()\n",
        "\n",
        "  # Run a validation loop at the end of each epoch.\n",
        "  for x_batch_val, y_batch_val in val_dataset:\n",
        "    val_logits = model(x_batch_val)\n",
        "    # Update val metrics\n",
        "    val_acc_metric(y_batch_val, val_logits)\n",
        "  val_acc = val_acc_metric.result()\n",
        "  val_acc_metric.reset_states()\n",
        "  print('Validation acc: %s' % (float(val_acc),))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rjraqJAonBJK"
      },
      "source": [
        "### Low-level handling of extra losses\n",
        "\n",
        "You saw in the previous section that it is possible for regularization losses to be added by a layer by calling `self.add_loss(value)` in the `call` method.\n",
        "\n",
        "In the general case, you will want to take these losses into account in your custom training loops (unless you've written the model yourself and you already know that it creates no such losses).\n",
        "\n",
        "Recall this example from the previous section, featuring a layer that creates a regularization loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fglnb_wzwtxh",
        "colab": {}
      },
      "source": [
        "class ActivityRegularizationLayer(layers.Layer):\n",
        "\n",
        "  def call(self, inputs):\n",
        "    self.add_loss(1e-2 * tf.reduce_sum(inputs))\n",
        "    return inputs\n",
        "\n",
        "inputs = keras.Input(shape=(784,), name='digits')\n",
        "x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
        "# Insert activity regularization as a layer\n",
        "x = ActivityRegularizationLayer()(x)\n",
        "x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
        "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "44zwXYXdwwss"
      },
      "source": [
        "When you call a model, like this:\n",
        "\n",
        "```python\n",
        "logits = model(x_train)\n",
        "```\n",
        "\n",
        "the losses it creates during the forward pass are added to the `model.losses` attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KOWuXgDlxIbS",
        "colab": {}
      },
      "source": [
        "logits = model(x_train[:64])\n",
        "print(model.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EvkOERFYymF_"
      },
      "source": [
        "The tracked losses are first cleared at the start of the model `__call__`, so you will only see the losses created during this one forward pass. For instance, calling the model repeatedly and then querying `losses` only displays the latest losses, created during the last call:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MZh9ek_NzEtQ",
        "colab": {}
      },
      "source": [
        "logits = model(x_train[:64])\n",
        "logits = model(x_train[64: 128])\n",
        "logits = model(x_train[128: 192])\n",
        "print(model.losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MmftRMi6zWLN"
      },
      "source": [
        "To take these losses into account during training, all you have to do is to modify your training loop to add `sum(model.losses)` to your total loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ueLhIG1mzdFp",
        "colab": {}
      },
      "source": [
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(x_batch_train)\n",
        "      loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "      # Add extra losses created during this forward pass:\n",
        "      loss_value += sum(model.losses)\n",
        "\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "    # Log every 200 batches.\n",
        "    if step % 200 == 0:\n",
        "        print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n",
        "        print('Seen so far: %s samples' % ((step + 1) * 64))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wCOe4e4dxsB0"
      },
      "source": [
        "That was the last piece of the puzzle! You've reached the end of this guide.\n",
        "\n",
        "Now you know everything there is to know about using built-in training loops and writing your own from scratch.\n"
      ]
    }
  ]
}